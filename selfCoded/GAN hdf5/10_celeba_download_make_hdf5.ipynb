{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "10_celeba_download_make_hdf5.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpII-qgno6EX",
    "colab_type": "text"
   },
   "source": [
    "# Human Faces - Download CelebA Data and Make HDF5\n",
    "\n",
    "Make Your First GAN With PyTorch, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi65ho8_tgQG",
    "colab_type": "text"
   },
   "source": [
    "## Download CelebA Dataset\n",
    "\n",
    "The downloaded data will be deleted after the colab virtual machine is deleted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jw2Il5l-zB1l",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2023-12-09T12:28:48.194116Z",
     "start_time": "2023-12-09T12:28:46.427515Z"
    }
   },
   "source": [
    "import torchvision.datasets"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-MKGmYJW1EPY",
    "colab_type": "code",
    "outputId": "a7d96fb7-8670-4dd9-f949-e31d424246bf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "ExecuteTime": {
     "end_time": "2023-12-06T20:49:28.993456Z",
     "start_time": "2023-12-06T20:49:28.331943Z"
    }
   },
   "source": [
    "# download data\n",
    "\n",
    "mnist_dataset = torchvision.datasets.CelebA(root='.', download=True)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# download data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m mnist_dataset \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mCelebA(root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/NeuronalNetwork/lib/python3.11/site-packages/torchvision/datasets/celeba.py:80\u001B[0m, in \u001B[0;36mCelebA.__init__\u001B[0;34m(self, root, split, target_type, transform, target_transform, download)\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_transform is specified but target_type is empty\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload()\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_integrity():\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/NeuronalNetwork/lib/python3.11/site-packages/torchvision/datasets/celeba.py:150\u001B[0m, in \u001B[0;36mCelebA.download\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (file_id, md5, filename) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_list:\n\u001B[0;32m--> 150\u001B[0m     download_file_from_google_drive(file_id, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_folder), filename, md5)\n\u001B[1;32m    152\u001B[0m extract_archive(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_folder, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg_align_celeba.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/NeuronalNetwork/lib/python3.11/site-packages/torchvision/datasets/utils.py:246\u001B[0m, in \u001B[0;36mdownload_file_from_google_drive\u001B[0;34m(file_id, root, filename, md5)\u001B[0m\n\u001B[1;32m    243\u001B[0m         api_response, content \u001B[38;5;241m=\u001B[39m _extract_gdrive_api_response(response)\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m api_response \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuota exceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 246\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    247\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe daily quota of the file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is exceeded and it \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    248\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be downloaded. This is a limitation of Google Drive \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    249\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand can only be overcome by trying again later.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    250\u001B[0m         )\n\u001B[1;32m    252\u001B[0m     _save_response_content(content, fpath)\n\u001B[1;32m    254\u001B[0m \u001B[38;5;66;03m# In case we deal with an unhandled GDrive API response, the file should be smaller than 10kB and contain only text\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9hyVvblv0kF",
    "colab_type": "text"
   },
   "source": [
    "## Extract Images and Re-Package as HDF5\n",
    "\n",
    "The HDF5 file is located in google Drive and won't be deleted when the colab virtual machine is deleted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mgPN34s04_li",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2023-12-09T12:28:52.009260Z",
     "start_time": "2023-12-09T12:28:51.725375Z"
    }
   },
   "source": [
    "import h5py\n",
    "import zipfile\n",
    "import imageio\n",
    "import os"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UJXhnB3wv7kl",
    "colab_type": "code",
    "outputId": "4131f301-1c89-4b21-a9e5-3eb2aff267f3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "ExecuteTime": {
     "end_time": "2023-12-09T12:30:45.882371Z",
     "start_time": "2023-12-09T12:28:54.543452Z"
    }
   },
   "source": [
    "%%time\n",
    "\n",
    "# location of the HDF5 package, yours may be under /gan/ not /myo_gan/\n",
    "hdf5_file = '/Volumes/PortableSSD/Projekte/datasets/celeba_dataset/IMG/celeba_aligned_small.h5py'\n",
    "\n",
    "# how many of the 202,599 images to extract and package into HDF5\n",
    "total_images = 20000\n",
    "\n",
    "with h5py.File(hdf5_file, 'w') as hf:\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with zipfile.ZipFile('/Volumes/PortableSSD/Projekte/datasets/celeba_dataset/IMG/img_align_celeba.zip', 'r') as zf:\n",
    "      for i in zf.namelist():\n",
    "        if (i[-4:] == '.jpg'):\n",
    "          # extract image\n",
    "          ofile = zf.extract(i)\n",
    "          img = imageio.imread(ofile)\n",
    "          os.remove(ofile)\n",
    "\n",
    "          # add image data to HDF5 file with new name\n",
    "          hf.create_dataset('/Volumes/PortableSSD/Projekte/datasets/celeba_dataset/IMG/img_align_celeba/'+str(count)+'.jpg', data=img, compression=\"gzip\", compression_opts=9)\n",
    "          \n",
    "          count = count + 1\n",
    "          if (count%1000 == 0):\n",
    "            print(\"images done .. \", count)\n",
    "            pass\n",
    "            \n",
    "          # stop when total_images reached\n",
    "          if (count == total_images):\n",
    "            break\n",
    "          pass\n",
    "\n",
    "        pass\n",
    "      pass"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:16: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images done ..  1000\n",
      "images done ..  2000\n",
      "images done ..  3000\n",
      "images done ..  4000\n",
      "images done ..  5000\n",
      "images done ..  6000\n",
      "images done ..  7000\n",
      "images done ..  8000\n",
      "images done ..  9000\n",
      "images done ..  10000\n",
      "images done ..  11000\n",
      "images done ..  12000\n",
      "images done ..  13000\n",
      "images done ..  14000\n",
      "images done ..  15000\n",
      "images done ..  16000\n",
      "images done ..  17000\n",
      "images done ..  18000\n",
      "images done ..  19000\n",
      "images done ..  20000\n",
      "CPU times: user 1min 43s, sys: 6.44 s, total: 1min 50s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3O1cSNS2_mdC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [],
   "execution_count": 0,
   "outputs": []
  }
 ]
}
