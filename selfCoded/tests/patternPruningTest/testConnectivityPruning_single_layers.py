import collections
import copy
import time

import numpy as np
from torch import tensor
import torch

mask_list = [
    tensor([[1.0, 1.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 1.0], [0.0, 0.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 1.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 1.0, 1.0], [0.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 1.0, 0.0], [1.0, 1.0, 0.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [1.0, 0.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[1.0, 0.0, 1.0], [1.0, 1.0, 0.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [1.0, 0.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 1.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 1.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 0.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 0.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 1.0, 0.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 1.0]]),
    tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [1.0, 1.0, 1.0]]),
    tensor([[0.0, 1.0, 1.0], [1.0, 1.0, 0.0], [0.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [1.0, 0.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 1.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 0.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 0.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 1.0], [0.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 1.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 1.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 1.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 1.0, 0.0], [0.0, 0.0, 0.0], [1.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 1.0, 1.0], [0.0, 0.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 1.0, 0.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 0.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 0.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 1.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 1.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 0.0], [1.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0], [1.0, 0.0, 0.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0], [0.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 0.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 0.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 0.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 0.0, 1.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 0.0, 1.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 0.0, 1.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [0.0, 1.0, 1.0], [1.0, 1.0, 0.0]]),
    tensor([[0.0, 0.0, 0.0], [0.0, 1.0, 1.0], [1.0, 0.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [0.0, 1.0, 1.0], [0.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 1.0]]),
    tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 1.0, 1.0]]),
]

layer_list = [
    tensor(
        [
            [
                [
                    [-0.3514, 0.1935, 0.5844],
                    [-1.1654, -1.0381, 0.0899],
                    [-0.7725, -0.2671, -1.0232],
                ]
            ]
        ]
    ),
    tensor(
        [
            [
                [
                    [-0.1333, 1.6396, 0.2913],
                    [-0.6652, 2.0789, -0.2103],
                    [0.3390, -0.5364, -0.2636],
                ],
                [
                    [-0.6246, -0.6156, 0.1761],
                    [1.5701, -1.3476, 0.2665],
                    [-1.8607, -1.0233, -0.5004],
                ],
                [
                    [-0.3313, -0.5896, 0.8764],
                    [1.3688, 0.6095, -0.1263],
                    [-0.0198, 0.1413, 0.6177],
                ],
                [
                    [-0.3184, 2.1088, 0.5151],
                    [0.2565, -0.8364, -1.1884],
                    [-0.3340, -0.6356, 0.0259],
                ],
                [
                    [0.1566, -1.3082, 1.0011],
                    [0.9085, -0.2991, 0.2408],
                    [-0.6950, -0.0723, 2.1684],
                ],
                [
                    [-0.0495, -0.0808, -0.3910],
                    [-1.7582, 1.0926, 0.3531],
                    [1.2955, 0.6530, 0.5203],
                ],
            ]
        ]
    ),
    tensor(
        [
            [
                [
                    [-7.6650e-01, -1.2105e00, -1.8992e-01],
                    [-3.4752e-01, -3.6723e-02, 2.0741e-01],
                    [-3.3259e-01, -7.8288e-01, 4.8082e-01],
                ],
                [
                    [-1.2539e00, 1.8145e00, -1.6166e00],
                    [1.3295e00, 8.8304e-01, -1.6700e-01],
                    [2.0301e00, -7.0857e-01, 4.9362e-01],
                ],
                [
                    [-7.9235e-02, -9.4043e-01, -9.5987e-01],
                    [-8.4273e-01, -1.3033e00, -9.8134e-01],
                    [-9.4947e-01, 8.4206e-01, -6.4065e-01],
                ],
                [
                    [-3.0548e-01, -1.5695e00, 5.0304e-01],
                    [-7.3145e-01, 1.4745e00, -1.7263e00],
                    [-1.4604e00, -1.0981e-02, -1.1812e00],
                ],
                [
                    [5.0304e-01, -2.7271e-01, 3.1906e-01],
                    [9.8109e-01, -4.6713e-02, -8.5974e-01],
                    [-5.7618e-01, 2.2955e00, -3.1324e-01],
                ],
                [
                    [1.6306e00, -5.2332e-01, -1.2035e00],
                    [-1.1110e00, 2.5035e-03, 4.7070e-01],
                    [-2.4764e00, -6.4831e-01, 2.0978e-01],
                ],
                [
                    [-1.5783e00, 8.6801e-01, 8.2736e-01],
                    [-5.6816e-02, 1.0684e00, 1.4180e00],
                    [-1.1934e00, 1.9360e00, 1.2176e-01],
                ],
                [
                    [1.4110e00, -8.2114e-01, -2.6458e-01],
                    [-2.2650e00, -2.4444e-01, 5.2179e-01],
                    [1.1166e00, -9.4199e-01, -1.4584e-01],
                ],
                [
                    [-1.1955e00, -5.8878e-01, -1.0471e00],
                    [2.4916e-02, -1.9324e-01, 1.8378e00],
                    [-3.3542e-01, 1.1460e00, -1.4640e00],
                ],
                [
                    [6.1282e-01, -1.6431e00, -1.8921e-01],
                    [-7.9246e-01, -6.8562e-02, -1.8171e-01],
                    [1.3956e-01, 9.0000e-01, -2.1395e-01],
                ],
                [
                    [1.1531e00, 5.0791e-01, 1.0384e00],
                    [5.8199e-01, 5.7410e-01, 1.4510e-01],
                    [-4.9708e-01, 8.2061e-01, -1.9812e00],
                ],
                [
                    [4.7189e-01, 1.7202e-01, 9.8842e-01],
                    [1.1604e00, 1.0583e00, 2.0290e00],
                    [-4.5997e-01, 1.6602e00, 3.0377e-01],
                ],
                [
                    [4.2868e-01, 3.1396e-02, 1.3032e00],
                    [-3.1103e-01, -2.6028e-01, 2.0817e-03],
                    [7.7295e-02, -1.0339e00, -4.4204e-01],
                ],
                [
                    [2.0595e-01, -1.9385e00, 1.7980e00],
                    [-1.1555e00, 1.3241e-02, 2.9890e-01],
                    [1.7605e00, 3.6084e-01, -1.6990e-01],
                ],
                [
                    [2.4824e00, -1.6490e00, 1.7950e00],
                    [-6.0749e-01, -1.6654e-01, 1.0565e00],
                    [4.1116e-01, -1.1635e00, 9.2963e-01],
                ],
                [
                    [-3.1086e-01, 2.4597e-01, -1.1016e00],
                    [5.3480e-01, -1.0356e00, 1.6962e00],
                    [-1.0093e00, 1.1851e00, 6.9365e-01],
                ],
            ],
            [
                [
                    [1.4331e-03, 1.6224e00, -2.5629e00],
                    [9.4572e-01, 2.7103e-01, 3.1272e-01],
                    [7.8954e-01, 4.2034e-01, -4.5731e-01],
                ],
                [
                    [6.2845e-01, -2.2977e-01, -1.2778e00],
                    [1.4891e00, 7.2242e-01, 1.2914e00],
                    [1.2523e00, 1.8436e00, 1.9829e00],
                ],
                [
                    [-1.0881e00, -8.5206e-01, -1.5961e00],
                    [-1.5838e00, -5.2683e-01, 1.1162e00],
                    [-2.3280e00, 1.2856e00, -6.4422e-01],
                ],
                [
                    [-4.9451e-01, -2.7118e-01, 2.0677e00],
                    [9.8141e-01, -3.2824e-02, 2.2475e-01],
                    [1.0420e00, 8.8239e-01, -1.1388e-01],
                ],
                [
                    [-5.2594e-01, -9.9968e-02, -2.1458e00],
                    [1.9139e00, 5.3143e-01, 6.3406e-01],
                    [-1.1302e-01, -8.5879e-01, -1.0783e00],
                ],
                [
                    [-1.9564e00, 6.4944e-01, -2.2030e00],
                    [-5.5181e-01, 1.0987e00, 1.4030e00],
                    [7.9034e-02, -8.5222e-01, 4.4627e-01],
                ],
                [
                    [-2.8671e-01, 1.3814e00, 1.8926e00],
                    [1.6840e00, -1.7948e-01, -1.5230e00],
                    [-1.0005e-01, -1.5125e-01, 2.1493e-01],
                ],
                [
                    [-8.5950e-01, -4.6745e-01, -2.1886e-01],
                    [5.2584e-01, -1.1296e00, 4.8156e-01],
                    [1.2104e00, 9.5930e-01, 1.1935e00],
                ],
                [
                    [-3.1396e-01, -2.4120e-01, 7.1240e-01],
                    [1.0998e-01, -6.3439e-01, 8.8791e-01],
                    [-4.9902e-02, -5.6663e-01, 4.2146e-01],
                ],
                [
                    [-9.5869e-01, -1.4568e00, 3.4975e-01],
                    [-1.2827e00, 6.4692e-01, -5.9773e-01],
                    [-6.9900e-02, -7.7035e-01, 1.2313e00],
                ],
                [
                    [-5.3332e-02, -1.6039e-02, 1.4554e00],
                    [-1.6061e00, -6.5638e-01, 6.7349e-01],
                    [-7.9357e-02, 6.3489e-01, -7.5633e-01],
                ],
                [
                    [-1.3536e00, 1.5116e00, -3.6261e-01],
                    [5.8713e-02, -5.3694e-02, -2.5967e-01],
                    [5.1205e-01, -1.7148e00, -9.6107e-01],
                ],
                [
                    [-4.7950e-01, -3.3409e-01, -3.4642e-01],
                    [-1.0077e00, -4.2118e-01, 1.0800e00],
                    [2.8513e-01, 5.5195e-02, 1.3067e00],
                ],
                [
                    [1.4143e00, 5.0824e-01, -2.7540e00],
                    [-8.7806e-01, -4.4693e-01, -3.6772e-01],
                    [3.2234e-01, -3.5036e-02, -1.1805e00],
                ],
                [
                    [1.1938e00, 1.2179e00, 1.7043e00],
                    [-8.2122e-01, -7.0600e-01, 8.2619e-01],
                    [2.9233e-01, 3.7311e-01, 1.5110e00],
                ],
                [
                    [-8.1524e-01, -4.9891e-01, -1.5800e-01],
                    [-2.5601e-01, -1.0772e00, -2.5133e00],
                    [1.3045e00, -7.5548e-01, -1.2173e00],
                ],
            ],
            [
                [
                    [2.6402e-01, -5.8310e-01, -1.5735e00],
                    [3.5393e-02, -1.1739e00, -1.5964e00],
                    [-5.7089e-02, 1.4072e00, 6.9158e-01],
                ],
                [
                    [-9.9505e-02, -4.2307e-01, -7.7516e-02],
                    [-8.1924e-02, 6.4024e-02, 1.0990e-01],
                    [-1.3792e00, 7.2066e-01, -9.2162e-01],
                ],
                [
                    [4.1790e-01, 6.1542e-01, -1.3005e00],
                    [-1.2945e00, -5.1059e-01, -8.9162e-01],
                    [6.8313e-01, 4.7597e-01, 1.1831e00],
                ],
                [
                    [5.3860e-01, -6.7381e-01, 1.4538e00],
                    [-9.5319e-01, 2.0292e00, 4.3664e-01],
                    [-2.0305e00, -8.8577e-02, 1.5425e00],
                ],
                [
                    [-5.8350e-01, -2.5901e-01, 4.8757e-01],
                    [-2.3766e00, -2.2203e-01, 9.2959e-01],
                    [-8.4211e-02, -1.4739e00, -1.4962e00],
                ],
                [
                    [3.6620e-01, 5.5757e-01, -3.9028e-01],
                    [2.4608e00, -1.2307e00, 5.9387e-02],
                    [-1.2548e00, 1.6322e00, -4.6818e-01],
                ],
                [
                    [-9.8103e-01, -1.1278e00, 1.4477e-01],
                    [1.1245e00, -7.5500e-01, -1.2781e-02],
                    [-2.1970e00, 7.4595e-01, -2.4354e00],
                ],
                [
                    [3.4066e-01, 5.2929e-01, -8.4980e-01],
                    [1.1589e00, 3.2977e-01, 7.5454e-02],
                    [-5.3494e-01, 1.6828e00, 1.7742e00],
                ],
                [
                    [-6.6684e-01, 1.5445e-01, 8.5186e-01],
                    [-1.4928e00, -1.3139e-01, 5.4668e-01],
                    [6.2919e-01, 3.3978e-01, 9.3245e-01],
                ],
                [
                    [-2.1392e-01, 7.1455e-01, 3.1956e00],
                    [-4.4557e-01, -4.4783e-01, 5.8625e-02],
                    [8.1544e-01, -3.2354e00, 7.8661e-01],
                ],
                [
                    [-1.7348e00, 6.9821e-01, 1.0726e-01],
                    [1.8073e00, 9.2955e-01, -7.9556e-01],
                    [6.1754e-01, -1.3673e00, -6.3007e-01],
                ],
                [
                    [1.0871e00, 1.1175e-01, 2.8752e-01],
                    [8.1084e-01, 2.6700e00, 4.2109e-01],
                    [1.2297e00, -3.0552e-01, -6.5751e-01],
                ],
                [
                    [-1.4364e00, 8.0900e-01, 1.1341e00],
                    [-4.7785e-01, -2.2319e-01, -1.0186e-01],
                    [5.0100e-01, 6.9990e-01, -1.1133e-02],
                ],
                [
                    [-2.7592e-01, 3.4490e-01, -1.3999e00],
                    [-4.5225e-01, 9.1178e-01, -1.2354e00],
                    [4.1326e-01, -4.1029e-01, 4.7111e-01],
                ],
                [
                    [9.6816e-01, -2.7124e-01, 1.3178e00],
                    [5.8406e-01, -1.1459e00, -6.6963e-01],
                    [-1.2562e00, 6.9833e-01, 8.9875e-01],
                ],
                [
                    [-6.3836e-01, -2.0557e-02, -7.4379e-01],
                    [-7.2711e-01, -1.3622e00, -1.6287e00],
                    [-5.5585e-01, -6.0943e-01, -1.1586e-01],
                ],
            ],
            [
                [
                    [-4.0162e-01, 4.7530e-01, -1.1006e00],
                    [-1.2903e00, -4.3981e-01, 9.5460e-01],
                    [-7.6007e-01, 9.2117e-01, -1.5870e00],
                ],
                [
                    [5.7295e-01, -2.2562e-01, 2.4561e-01],
                    [1.7831e00, 9.6293e-02, -2.9324e00],
                    [1.0654e00, -9.6511e-01, -1.6760e-01],
                ],
                [
                    [2.4265e-02, -1.7830e00, 3.4606e-01],
                    [8.6489e-02, 1.5657e-01, -7.4010e-01],
                    [1.2870e00, -1.7674e00, -9.9253e-02],
                ],
                [
                    [5.3399e-01, 7.9176e-01, 1.8573e00],
                    [2.4182e00, 1.0784e00, -1.1049e00],
                    [4.1184e-02, -1.4247e-01, -8.6668e-01],
                ],
                [
                    [1.3982e00, -4.1274e-01, 4.8356e-01],
                    [-2.0865e-01, 7.4529e-01, 1.0269e00],
                    [5.8118e-01, -6.8742e-01, -7.0609e-01],
                ],
                [
                    [1.8094e00, 5.6063e-02, -6.2073e-01],
                    [1.1235e00, -6.0904e-01, -1.2638e00],
                    [1.0194e00, 1.3987e00, 8.0355e-01],
                ],
                [
                    [3.9567e-01, -1.9598e-01, 1.6861e00],
                    [-2.0912e00, -2.8768e-01, 2.1164e00],
                    [9.4005e-01, -5.1268e-01, -1.3928e00],
                ],
                [
                    [-1.6634e-01, 3.8291e-01, 2.5958e-01],
                    [-9.7437e-01, -4.0920e-01, -1.6772e00],
                    [-4.1209e-01, -1.6080e-01, 5.7197e-01],
                ],
                [
                    [-2.0118e00, 2.4455e-01, 1.7320e00],
                    [1.0854e00, 1.2056e00, -9.6088e-01],
                    [3.2370e-02, 4.1805e-01, -1.2785e00],
                ],
                [
                    [9.9707e-01, -1.6776e00, -2.5717e-01],
                    [-1.3765e00, -6.7906e-01, -5.7646e-02],
                    [-8.9661e-02, -6.1437e-01, 2.2181e00],
                ],
                [
                    [-8.5471e-01, -9.1885e-01, -1.4767e00],
                    [-4.0081e-01, -8.9298e-01, -6.9928e-01],
                    [-4.7052e-01, -1.3844e00, -1.0257e00],
                ],
                [
                    [4.6155e-01, -3.3438e-01, 8.2418e-01],
                    [-9.2201e-02, -5.4420e-01, -1.3170e00],
                    [-3.8535e-01, -2.0679e00, -1.0982e00],
                ],
                [
                    [-1.9191e00, -5.1652e-01, 7.7290e-01],
                    [5.0245e-01, 9.7033e-01, -1.7417e00],
                    [1.8923e00, -7.7389e-01, 1.9849e-01],
                ],
                [
                    [3.3133e-01, 3.4425e-01, -5.2689e-01],
                    [-3.3795e-01, 8.5468e-01, 3.6763e-01],
                    [5.8924e-01, 1.3966e00, -2.5015e00],
                ],
                [
                    [3.9841e-01, 9.8077e-01, -7.4664e-01],
                    [-3.3905e-01, 8.8385e-01, -2.5231e00],
                    [-1.0777e00, 1.4549e00, -4.5262e-01],
                ],
                [
                    [1.3789e00, 6.0084e-01, 3.1985e00],
                    [1.3211e00, -2.8378e-01, -9.6529e-01],
                    [5.1699e-01, 4.7649e-01, 1.0825e00],
                ],
            ],
            [
                [
                    [1.2043e-01, 1.0919e-01, 1.0005e00],
                    [7.0196e-01, -4.1169e-01, -2.5099e00],
                    [4.5771e-01, 1.6122e00, -1.2865e00],
                ],
                [
                    [-9.9879e-01, -4.6085e-01, -1.6520e-01],
                    [-1.1819e00, 3.4102e-01, 6.8138e-01],
                    [-6.9525e-02, -1.3680e-01, -5.0552e-02],
                ],
                [
                    [-6.3761e-01, -3.6856e-01, -8.0335e-01],
                    [-1.1182e-01, 1.3275e00, -6.2600e-01],
                    [1.0444e00, -2.7810e-01, 1.5534e-01],
                ],
                [
                    [-2.4865e-01, 2.1671e-01, -2.5583e-01],
                    [1.4071e00, 1.1453e00, -9.0234e-01],
                    [-5.7569e-02, -1.8890e00, 1.7290e00],
                ],
                [
                    [-7.9518e-01, 5.5707e-01, -5.2407e-01],
                    [-4.7807e-01, -8.8948e-01, 2.0387e00],
                    [1.9206e00, 2.3685e-02, -1.3021e00],
                ],
                [
                    [-2.9270e-01, 8.0906e-01, -1.6621e-01],
                    [-1.4384e00, 4.6217e-01, -1.9733e00],
                    [-7.3957e-01, 1.6430e-01, 1.0759e00],
                ],
                [
                    [-9.2224e-01, 4.9402e-01, 6.3952e-01],
                    [-7.1763e-01, 1.2117e00, -1.4113e00],
                    [-1.1501e00, -2.4178e-01, -2.5762e-01],
                ],
                [
                    [-8.7934e-01, -4.3663e-02, 1.9935e-01],
                    [-9.4552e-01, 2.0283e-01, 4.7743e-01],
                    [-8.9381e-02, 1.5683e00, -5.0163e-01],
                ],
                [
                    [9.2600e-01, 1.6821e-01, 4.3732e-02],
                    [-1.4573e-01, -3.1223e-01, -4.1425e-01],
                    [-4.9696e-01, -8.3854e-03, 8.4576e-01],
                ],
                [
                    [2.5164e-01, -1.4669e00, 5.1621e-01],
                    [-3.1457e-01, 2.2611e00, 1.5684e00],
                    [1.1630e00, 1.5439e00, 1.7893e00],
                ],
                [
                    [-7.3790e-02, 3.4036e-02, 1.9243e-01],
                    [1.2631e-01, -6.5648e-01, 1.1899e-01],
                    [-1.8808e-01, 2.2669e-01, 1.0648e00],
                ],
                [
                    [1.5770e00, -2.1380e00, -8.9882e-01],
                    [-1.1332e00, -1.4135e00, 2.1555e00],
                    [-5.4156e-01, 4.8600e-01, -1.8294e00],
                ],
                [
                    [-8.5198e-01, -8.9801e-01, -4.1101e-01],
                    [-6.7152e-02, -3.6634e-01, 2.4068e-01],
                    [-1.4826e00, 7.0573e-01, -2.1362e00],
                ],
                [
                    [-1.3681e00, 1.5761e00, 3.5089e-01],
                    [-6.5481e-01, 1.9535e-01, 5.6895e-01],
                    [1.5266e00, 1.4089e-02, -1.0813e00],
                ],
                [
                    [-7.4089e-01, -4.1334e-01, -1.4973e00],
                    [2.3314e-01, -2.2354e-01, -6.4118e-01],
                    [4.2626e-01, -1.8513e00, 2.0427e-01],
                ],
                [
                    [4.0491e-01, 7.5737e-02, -6.7324e-01],
                    [-1.5308e00, 6.6770e-02, -4.4356e-01],
                    [1.0869e00, 2.7705e-01, -1.2917e00],
                ],
            ],
            [
                [
                    [-4.5342e-01, -5.6086e-01, -8.3969e-01],
                    [5.1699e-02, -1.9923e-01, 5.0325e-01],
                    [1.9941e00, -4.2069e-01, -7.4842e-01],
                ],
                [
                    [4.7171e-01, 1.9849e00, -4.0272e-01],
                    [2.9096e-01, 5.3809e-01, 2.6566e-01],
                    [7.8547e-02, 9.6211e-01, -6.0745e-01],
                ],
                [
                    [9.6035e-01, 2.4562e-01, -1.0822e00],
                    [8.2028e-01, -1.0594e00, -2.6146e00],
                    [-1.7083e00, 8.8011e-01, 6.1632e-01],
                ],
                [
                    [2.8937e-01, -2.8602e-01, 1.4909e00],
                    [6.6682e-01, 8.2739e-01, 3.2002e-01],
                    [4.5214e-01, 1.4772e00, -4.1757e-01],
                ],
                [
                    [-6.3193e-01, -6.8045e-01, -9.2232e-01],
                    [5.6275e-01, -6.7292e-02, -8.4442e-01],
                    [-1.5350e00, 4.2061e-01, -5.0654e-01],
                ],
                [
                    [-1.4733e00, -3.2109e-01, -6.1275e-01],
                    [7.5563e-01, -6.3048e-01, -6.1039e-01],
                    [1.6227e00, -1.0299e00, 3.3459e-01],
                ],
                [
                    [1.1648e00, 1.9652e00, -2.3021e-01],
                    [-1.6862e00, 8.4949e-01, 8.7139e-01],
                    [2.4513e-01, 4.1598e-02, 4.7041e-01],
                ],
                [
                    [-7.8103e-01, 1.4051e-01, 4.9907e-01],
                    [5.2796e-01, 2.0276e00, -4.5161e-01],
                    [-6.8949e-01, -8.2379e-01, -9.5320e-01],
                ],
                [
                    [2.3690e-01, 6.3153e-01, -1.4900e-01],
                    [-1.0143e00, -7.4613e-01, 2.3937e-01],
                    [-8.8985e-01, 3.3957e-01, -2.3667e00],
                ],
                [
                    [-9.2111e-01, 1.6586e-02, 1.0031e00],
                    [6.0073e-01, 1.1652e-01, -7.0607e-01],
                    [4.8987e-01, 1.0145e00, -1.4254e00],
                ],
                [
                    [1.6535e00, -2.2083e00, -1.2765e00],
                    [-1.0666e00, 9.3942e-01, 1.3959e-01],
                    [-6.8759e-01, 7.0287e-01, -1.1714e00],
                ],
                [
                    [-3.0013e-01, -5.7665e-02, -7.8642e-01],
                    [-4.0572e-01, 1.5993e00, -9.9212e-01],
                    [2.7820e-01, 2.1144e00, 4.2857e-01],
                ],
                [
                    [-1.5019e00, 4.0222e-01, -1.4894e00],
                    [-1.1293e00, -1.2877e00, -2.3763e-01],
                    [6.3861e-01, 3.0545e00, 1.8364e-01],
                ],
                [
                    [1.5839e00, -8.1912e-01, 1.3218e00],
                    [-1.9516e00, 2.6632e-01, 1.0191e00],
                    [2.2039e00, 4.7082e-01, -1.7161e00],
                ],
                [
                    [-1.6467e00, -4.5793e-01, -1.3490e00],
                    [1.4787e00, -1.4924e00, -3.2737e-01],
                    [9.6271e-02, -7.7729e-01, -8.7178e-01],
                ],
                [
                    [1.1481e00, 2.1434e-02, -1.0120e00],
                    [-1.7217e00, -1.9779e-01, -2.2840e-01],
                    [1.0306e00, -3.2995e-01, 1.1902e00],
                ],
            ],
        ]
    ),
]

kernel_mask_index_reference_list = [
    [[115]],
    [[73, 114, 94, 82, 60, 114]],
    [
        [20, 58, 101, 81, 117, 23, 52, 43, 54, 9, 35, 112, 35, 58, 4, 102],
        [58, 119, 98, 98, 100, 26, 57, 124, 102, 10, 97, 20, 42, 25, 5, 122],
        [102, 90, 97, 105, 119, 114, 79, 100, 25, 110, 38, 37, 4, 103, 27, 91],
        [97, 117, 87, 91, 48, 41, 97, 118, 29, 10, 70, 109, 46, 124, 87, 25],
        [109, 7, 27, 116, 122, 77, 46, 45, 53, 123, 106, 17, 19, 19, 31, 99],
        [69, 86, 101, 93, 65, 43, 7, 51, 115, 35, 5, 102, 28, 44, 21, 44],
    ],
]
kernel_mask_index_reference_list_copy = copy.deepcopy(kernel_mask_index_reference_list)

# Berechnung der Frobenius-Norm f端r jeden Kernel und die entsprechende Maske
frob_list = copy.deepcopy(kernel_mask_index_reference_list)
for layer_index, layer in enumerate(layer_list):
    for channel_index, channel in enumerate(layer):
        for kernel_index, kernel in enumerate(channel):
            # Referenzindex f端r die aktuelle Maske aus der Referenzliste extrahieren
            mask_indices = kernel_mask_index_reference_list[layer_index][channel_index][kernel_index]
            # F端r jede Maske die Frobenius-Norm berechnen und in frobenius_list speichern

            mask = mask_list[mask_indices]
            frob_list[layer_index][channel_index][kernel_index] = float(torch.norm(kernel * mask, p='fro'))


ratio = 0.1

#print(frob_list)
# def set_frob_list_values(frob_list, index_ref, ratio=0.8):
#     # Schritt 1: Flachlegen der Liste
#     flat_list = [frob for layer in frob_list for channel in layer for frob in channel]
#
#     # Schritt 2: Schwellenwert finden
#     threshold_index = int(len(flat_list) * ratio)
#     sorted_values = sorted(flat_list)
#     threshold_value = sorted_values[threshold_index]
#
#     # Schritt 3: Urspr端ngliche Liste aktualisieren
#     for layer_index, layer in enumerate(frob_list):
#         for channel_index, channel in enumerate(layer):
#             for kernel_index, _ in enumerate(channel):
#                 if frob_list[layer_index][channel_index][kernel_index] < threshold_value:
#                     # frob_list[layer_index][channel_index][kernel_index] = 0
#                     index_ref[layer_index][channel_index][kernel_index] = None
#                 # else:
#                 #     frob_list[layer_index][channel_index][kernel_index] = 1
#
#     # print(f'counter of set_frob_list_values: {counter}')
#
# set_frob_list_values(frob_list, kernel_mask_index_reference_list, ratio)
# print(frob_list)


# def update_indices_based_on_frobenius_norm_per_layer(layer_list, mask_list, indices_list, threshold_ratios):
#     updated_indices_list = copy.deepcopy(indices_list)
#
#     for layer_idx, (layer, threshold_ratio) in enumerate(zip(layer_list, threshold_ratios)):
#         layer_norms = []
#         positions = []
#
#         # Compute Frobenius norms for the current layer
#         for channel_idx, kernels in enumerate(layer):
#             for kernel_idx, kernel in enumerate(kernels):
#                 mask_index = indices_list[layer_idx][channel_idx][kernel_idx]
#                 if mask_index is not None:  # Ensure the mask index is valid
#                     mask = mask_list[mask_index]
#                     norm = torch.norm(kernel * mask, p='fro').item()
#                     layer_norms.append(norm)
#                     positions.append((layer_idx, channel_idx, kernel_idx))
#
#         # Sort norms within this layer to find the layer-specific threshold
#         sorted_norms_and_positions = sorted(zip(layer_norms, positions), key=lambda x: x[0])
#         threshold_index = round(len(layer_norms) * threshold_ratio)
#         threshold_value = sorted_norms_and_positions[threshold_index][0] if sorted_norms_and_positions else float('inf')
#         #print(sorted_norms_and_positions)
#
#
#
#         # Apply the layer-specific threshold
#         for norm, (layer_idx, channel_idx, kernel_idx) in sorted_norms_and_positions:
#             if norm < threshold_value:
#                 updated_indices_list[layer_idx][channel_idx][kernel_idx] = None
#
#     return updated_indices_list

def update_indices_based_on_frobenius_norm_per_layer(layer_list, mask_list, indices_list, threshold_ratios):
    assert len(layer_list) == len(threshold_ratios), "Mismatch between number of layers and number of threshold ratios."

    # comment this line out if you want ot enable inline configuration
    updated_indices_list = copy.deepcopy(indices_list)

    # Process each layer with its corresponding pruning threshold
    for layer_idx, (layer, threshold_ratio) in enumerate(zip(layer_list, threshold_ratios)):
        norms = []

        # Collect Frobenius norms for all kernels in the layer
        for channel_idx, kernels in enumerate(layer):
            for kernel_idx, _ in enumerate(kernels):
                mask_idx = indices_list[layer_idx][channel_idx][kernel_idx]
                if mask_idx is not None:  # Ensure the kernel hasn't been pruned already
                    mask = mask_list[mask_idx]
                    norm = torch.norm(kernels[kernel_idx] * mask, p='fro').item()
                    norms.append((norm, layer_idx, channel_idx, kernel_idx))

        # Determine how many kernels to prune based on the specified ratio
        num_to_prune = round(len(norms) * threshold_ratio)
        # Sort norms to identify which kernels have the lowest norms
        norms_sorted = sorted(norms, key=lambda x: x[0])

        # Prune the specified percentage of kernels with the lowest norms
        for _, l_idx, c_idx, k_idx in norms_sorted[:num_to_prune]:
            updated_indices_list[l_idx][c_idx][k_idx] = None


    return updated_indices_list



# Example usage:
threshold_ratios = [0.9, 0.6/6, 0.25]  # print("-------------")Example threshold ratios for each layer
updated_kernel_mask_index_reference_list = update_indices_based_on_frobenius_norm_per_layer(
    layer_list, mask_list, kernel_mask_index_reference_list, threshold_ratios)

#print(layer_list)
print("-------------")
for frob, indice, kernel in zip(frob_list, updated_kernel_mask_index_reference_list, kernel_mask_index_reference_list):
    none_counter = 0
    all_elem = 0
    for frob_, indice_, kernel_ in zip(frob,indice,kernel):
        for frob_i, indice_i, kernel_i in zip(frob_, indice_, kernel_):
            all_elem +=1
            if indice_i is None:
                none_counter += 1
            print(frob_i)
            print(indice_i)
            print(kernel_i)
        print("-------------")
    print(f"All Elements number: {all_elem}")
    print(f"None Elements number: {none_counter}")
print(updated_kernel_mask_index_reference_list)
#----------------------------------

# def count_occurrences_iterative(nested_list, value):
#     '''
#     counts the occurence of value inside of the nested_list and returns the count
#     '''
#     count = 0
#     queue = collections.deque([nested_list])
#
#     while queue:
#         current = queue.popleft()
#         for item in current:
#             if item == value:
#                 count += 1
#             elif isinstance(item, list):
#                 queue.append(item)
#     return count
#
# counter = 0
# for i in range(len(mask_list)):
#     temp = count_occurrences_iterative(kernel_mask_index_reference_list, i)
#     counter += temp
#     print(f"Index {i} counts {temp}")
#
# print(counter)

def convert_to_single_tensor(tensor_assignments, pattern_library, layer_index):
    # Directly access the specific layer's indices
    sublist = tensor_assignments[layer_index]

    # Convert indices to a PyTorch tensor for advanced indexing
    if len(sublist[0]) == 1:  # Shape (1,1,3,3)
        # Flatten the sublist since it contains single-item lists
        indices = [idx[0] for idx in sublist]
        tensor = torch.stack([pattern_library[i] if i is not None else torch.zeros(3, 3)
                              for i in indices]).unsqueeze(1)
    else:  # Shape (6,16,3,3)
        # For more complex case, use advanced indexing if possible
        layer_tensors = []
        for group in sublist:
            # Convert group to tensor for advanced indexing
            group_indices = [idx for idx in group]#torch.tensor(group)
            group_tensor = torch.stack([pattern_library[i] if i is not None else torch.zeros(3, 3)
                                        for i in group_indices])
            layer_tensors.append(group_tensor)
        tensor = torch.stack(layer_tensors)

    return tensor

print(convert_to_single_tensor(updated_kernel_mask_index_reference_list, mask_list, 2))
# print(convert_to_single_tensor(updated_kernel_mask_index_reference_list, mask_list, 1))



#[f(x) if condition else g(x) for x in sequence]





